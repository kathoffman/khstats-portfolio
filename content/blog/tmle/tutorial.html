---
title: "An Illustrated Guide to TMLE, Part I: Introduction and Motivation"
author: "Katherine Hoffman"
date: 2020-12-11T00:13:14-05:00
draft: false
weight: 8
image: /img/tmle/parametric_assumptions_comic.jpg
categories: ["statistics","R"]
math: true
tags: ["statistics","causal inference","R","TMLE","Targeted Maximum Likelihood Estimation","introduction to TMLE","targeted learning"]
output:
  blogdown::html_page:
    toc: false
    toc_depth: 1
---



<p>The introductory post of a three-part series to help beginners understand Targeted Maximum Likelihood Estimation (TMLE). This section contains a brief overview of the targeted learning framework and motivation for semiparametric estimation methods for inference, including causal inference.</p>
<!--more-->
<p><strong><em>December 6, 2020</em></strong></p>
<blockquote>
<p>The <strong>introductory post</strong> of a three-part series to help beginners understand Targeted Maximum Likelihood Estimation (TMLE). This section contains a brief overview of the <strong>targeted learning framework</strong> and motivation for <strong>semiparametric estimation methods for inference</strong>, including causal inference.</p>
</blockquote>
<hr />
<div id="table-of-contents" class="section level1">
<h1>Table of Contents</h1>
<p><em>This blog post series has three parts:</em></p>
<div id="part-i-motivation" class="section level3">
<h3>Part I: Motivation</h3>
<ol style="list-style-type: decimal">
<li><a href="#tmle-in-three-sentences">TMLE in three sentences 🎯</a></li>
<li><a href="#an-analysts-motivation-for-learning-tmle">An Analyst’s Motivation for Learning TMLE 👩🏼‍💻</a></li>
<li><a href="#is-tmle-causal-inference">Is TMLE Causal Inference? 🤔</a></li>
</ol>
</div>
<div id="part-ii-algorithm" class="section level3">
<h3><a href="https://khstats.com/blog/tmle/tutorial-pt2">Part II: Algorithm</a></h3>
<ol start="4" style="list-style-type: decimal">
<li><a href="https://khstats.com/blog/tmle/tutorial-pt2/#why-the-visual-guide">Why the Visual Guide? 🎨</a></li>
<li><a href="https://khstats.com/blog/tmle/tutorial-pt2/#tmle-step-by-step">TMLE, Step-by-Step 🚶🏽</a></li>
<li><a href="https://khstats.com/blog/tmle/tutorial-pt2/#using-the-tmle-package">Using the <code>tmle</code> package 📦</a></li>
</ol>
</div>
<div id="part-iii-evaluation" class="section level3">
<h3><a href="https://khstats.com/blog/tmle/tutorial-pt3">Part III: Evaluation</a></h3>
<ol start="7" style="list-style-type: decimal">
<li><a href="https://khstats.com/blog/tmle/tutorial-pt3/#properties-of-tmle">Properties of TMLE 📈</a></li>
<li><a href="https://khstats.com/blog/tmle/tutorial-pt3/#why-does-tmle-work">Why does TMLE work? ✨</a></li>
<li><a href="https://khstats.com/blog/tmle/tutorial-pt3/#resources-to-learn-more">Resources to learn more 🤓</a></li>
</ol>
<hr />
</div>
</div>
<div id="tmle-in-three-sentences" class="section level1">
<h1>TMLE in three sentences 🎯</h1>
<p>Targeted Maximum Likelihood Estimation (TMLE) is a semiparametric estimation framework to <strong>estimate a statistical quantity of interest</strong>. TMLE allows the use of <strong>machine learning</strong> (ML) models which place <strong>minimal assumptions on the distribution of the data</strong>. Unlike estimates normally obtained from ML, the <strong>final TMLE estimate will still have valid standard errors for statistical inference</strong>.</p>
</div>
<div id="an-analysts-motivation-for-learning-tmle" class="section level1">
<h1>An Analyst’s Motivation for Learning TMLE 👩🏼‍💻</h1>
<p>When I graduated with my MS in Biostatistics two years ago, I had a mental framework of statistics and data science that I think is pretty common among new graduates. It went like this:</p>
<ol style="list-style-type: decimal">
<li><p>If the goal is <span style="color: #3366ff;">inference</span> (e.g., an effect size with a confidence interval), use an <span style="color: #3366ff;">interpretable, usually parametric, model</span> and explain what the coefficients and their standard errors mean.</p></li>
<li><p>If the goal is <span style="color: #cc0000;">prediction</span>, use <span style="color: #cc0000;">data-adaptive machine learning algorithms</span> and then look at performance metrics, with the understanding that standard errors, and sometimes even coefficients, no longer exist.</p></li>
</ol>
<p>This mentality changed drastically when I started learning about semiparametric estimation methods like TMLE in the context of causal inference. I quickly realized two flaws in this mental framework.</p>
<p>First, I was thinking about inference backwards: I was choosing a model based on my outcome type (binary, continuous, time-to-event, repeated measures) and then interpreting specific coefficients as my estimates of interest. Yet it makes way more sense to <em>first</em> determine the statistical quantity, or <strong>estimand</strong>, that best answers a scientific question, and <em>then</em> use the method, or <strong>estimator</strong>, best suited for estimating it. This is the paradigm TMLE is based upon: <strong>we want to build an algorithm, or estimator, targeted to an estimand of interest</strong>.</p>
<figure>
<img src="/img/tmle/estimator.png"
    width= 90%
         alt="Estimator and Estimand">
<figcaption>
<em>An estimand is a quantity that answers a scientific question of interest. Once we figure out the estimand, we can build an estimator, or algorithm, to estimate it. Image courtesy of Dr. Laura Hatfield and <a href="https://diff.healthpolicydatascience.org/">diff.healthpolicydatascience.org</a>.</em>
</figcaption>
</figure>
<p>Second, I thought flexible, data-adaptive models we commonly classify as statistical and/or <strong>machine learning</strong> (e.g. LASSO, random forests, gradient boosting, etc.) could only be used for prediction, since they don’t have <strong>asymptotic properties for inference</strong> (i.e. standard errors). However, certain <strong>semiparametric estimation methods</strong> like TMLE can actually use these models to <strong>obtain a final estimate that is closer to the target quantity</strong> than would be obtained using classic parametric models (e.g. linear and logistic regression). This is because machine learning models are generally designed to accommodate <strong>large numbers of covariates</strong> with <strong>complex, non-linear relationships</strong>.</p>
<img src="/img/tmle/parametric_assumptions_comic.jpg" width=100%>
<figcaption>
<em>Semiparametric estimation methods like TMLE can rely on machine learning to avoid making unrealistic parametric assumptions about the underlying distribution of the data (e.g. multivariate normality).</em>
</figcaption>
<p>The way we use the machine learning estimates in TMLE, surprisingly enough, yields <strong>known asymptotic properties of bias and variance</strong> – just like we see in parametric maximum likelihood estimation – for our target estimand.</p>
<p>Besides allowing us to compute 95% confidence intervals and p-values for our estimates even after using flexible models, TMLE achieves other beneficial statistical properties, such as <strong>double robustness</strong>. These are discussed further in <a href="https://khstats.com/blog/tmle/tutorial-pt3/"><em>Part III</em></a>.</p>
</div>
<div id="is-tmle-causal-inference" class="section level1">
<h1>Is TMLE Causal Inference? 🤔</h1>
<p>If you’ve heard about TMLE before, it was likely in the context of <strong>causal inference</strong>. Although TMLE was developed for causal inference due to its many attractive properties, it cannot be considered causal inference by itself. Causal inference is a two-step process that first requires <strong>causal assumptions</strong><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> before a statistical estimand can be interpreted causally.</p>
<p><strong>TMLE can be used to estimate various statistical estimands</strong> (odds ratio, risk ratio, mean outcome difference, etc.) <strong>even when causal assumptions are not met</strong>. TMLE is, as its name implies, simply a tool for estimation.</p>
<p>In <a href="https://khstats.com/blog/tmle/tutorial-pt2/"><em>Part II</em></a>, I’ll walk step-by-step through a basic version of the TMLE algorithm: estimating the mean difference in outcomes, adjusted for confounders, for a binary outcome and binary treatment. If causal assumptions are met, this is called the <strong>Average Treatment Effect (ATE)</strong>, or the mean difference in outcomes in a world in which everyone had received the treatment compared to a world in which everyone had not.</p>
<p>⤴️<a href="#table-of-contents"><em>Back to the top</em></a></p>
<p>➡️<a href="https://khstats.com/blog/tmle/tutorial-pt2/"><em>Continue to Part II: The Algorithm</em></a></p>
<hr />
<div id="references" class="section level3">
<h3><em>References</em></h3>
<p>My primary reference for all three posts is <a href="https://link.springer.com/book/10.1007/978-1-4419-9782-1"><em>Targeted Learning</em></a> by Mark van der Laan and Sherri Rose. I detail many other resources I’ve used to learn TMLE, semiparametric theory, and causal inference in <a href="https://khstats.com/blog/tmle/tutorial-pt3/"><em>Part III</em></a>.</p>
</div>
</div>
<div class="footnotes footnotes-end-of-document">
<hr />
<ol>
<li id="fn1"><p>I won’t discuss causal assumptions in these posts, but this is referring to fundamental assumptions in causal inference like consistency, exchangeability, and positivity. A primary motivation for using TMLE and other semiparametric estimation methods for causal inference is that if you’ve already taken the time to carefully evaluate <em>causal</em> assumptions, it does not make sense to then damage an otherwise well-designed analysis by making unrealistic <em>statistical</em> assumptions.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
